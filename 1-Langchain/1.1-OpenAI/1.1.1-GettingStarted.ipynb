{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d66ff85f",
   "metadata": {},
   "source": [
    "Getting Started with Building Gen AI applications\n",
    "1. Building a simple application with LangChain\n",
    "2. Trace our application with LangSmith\n",
    "3. Serve our application with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed0ee02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Ensure the environment variables are loaded\n",
    "os.environ[\"OPENAI_API_KEY\"] = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af1eb5d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002E0BB13D2B0>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002E0BB13DD30>, root_client=<openai.OpenAI object at 0x000002E0B50C2900>, root_async_client=<openai.AsyncOpenAI object at 0x000002E0BB13DA90>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI # Import the OpenAI chat model\n",
    "llm=ChatOpenAI(model=\"gpt-4o\") # Initialize the language model\n",
    "llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91cedcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='The capital of France is Paris.' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 14, 'total_tokens': 21, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-C0az79Gydty5JH2b5SPPmfj59lntw', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--6d48ca4b-81ed-4658-bc0f-b3ec246612d7-0' usage_metadata={'input_tokens': 14, 'output_tokens': 7, 'total_tokens': 21, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "## Querying the Model now.\n",
    "result=llm.invoke(\"what is the capital of France?\")\n",
    "print(result) # Print the result of the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b5b100",
   "metadata": {},
   "source": [
    "Prompt Responses using ChatPrompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ddb455b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content=\"Sure! A chat prompt template is like a guide or a set of instructions that helps shape how a conversation with an AI proceeds. When you're chatting with an AI, the prompt is the initial input that sets the context and directs the AI in how to respond. Here’s a simple breakdown:\\n\\n1. **Framework for Conversations**: The template acts as a framework to structure interactions with the AI. It decides the initial setup or tone, which can be informative, casual, direct, supportive, or any other style that the designer chooses.\\n\\n2. **Guiding the AI**: It includes specific instructions or key elements that guide the AI's responses, indicating what kind of information to include or exclude. For instance, it might tell the AI to focus on answering questions concisely or to elaborate on its responses.\\n\\n3. **Consistency**: Using a template ensures that the responses are consistent in style and content, which is important for maintaining a coherent and predictable interaction, especially if the AI is being used for customer service or educational purposes.\\n\\n4. **Customization**: Different scenarios might require different prompt templates. For instance, a customer support AI might have a template that focuses on troubleshooting and empathy, while a code assistant might have a template that encourages technical precision and examples.\\n\\nBy employing chat prompt templates, developers and designers can control how an AI interacts, aiming to make conversations more meaningful, efficient, and suited to the user's needs.\" additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 287, 'prompt_tokens': 41, 'total_tokens': 328, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-C0b13A6cOZVNgIa5Qw1O3pASacF8x', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None} id='run--8bdfbbcf-0ee7-48f3-be81-2ad1a48020f5-0' usage_metadata={'input_tokens': 41, 'output_tokens': 287, 'total_tokens': 328, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate # Import the chat prompt template\n",
    "\n",
    "prompt=ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\",\"You are an AI assistant , provide response based on the question.\"),\n",
    "        (\"user\",\"{input}\")\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create a prompt template with the specified structure\n",
    "chain=prompt|llm # Combine the prompt template with the language model\n",
    "response=chain.invoke({\"input\":\"Can you please explain to a Computer science student, about Chat prompt Template in simpler words\"}) \n",
    "# Print the result of the chain invocation\n",
    "print(response) \n",
    "# Print the result of the chain invocation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d2ccee8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "langchain_core.messages.ai.AIMessage"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f92ec53",
   "metadata": {},
   "source": [
    "If you observe, the type of Above response is \"AI Message\", \n",
    "we have other class called StrOutputParser(), where we can customize the output and get the output response as \"String\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b4342f35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Certainly! A chat prompt template is a preset structure or pattern that helps generate responses in a conversational AI, like chatbots. \n",
      "\n",
      "Imagine you're trying to carry on a conversation with a computer program. The chat prompt template acts like a “starter pack” for the conversation. It defines how the initial message should look, what elements it should include, and often helps guide the AI to provide relevant and coherent answers.\n",
      "\n",
      "Here's how it works in a simplified manner:\n",
      "\n",
      "1. **Structure**: The template provides an organized framework for formulating prompts. It might specify certain placeholders or sections, such as a greeting, the main context or question, and other details that set the stage for a response.\n",
      "\n",
      "2. **Consistency**: By using templates, responses can remain consistent in format and content, which is important for maintaining a smooth user experience.\n",
      "\n",
      "3. **Customization**: Templates can be adjusted or filled with specific details depending on the needs of the conversation. For example, a template might have a placeholder for the user's name or particular subject matter that changes with each interaction.\n",
      "\n",
      "4. **Efficiency**: Templates help save time and reduce errors by providing a ready-made starting point rather than composing each response from scratch.\n",
      "\n",
      "In essence, chat prompt templates are a way to standardize and streamline interactions to make them predictable, efficient, and relevant, enhancing the overall quality of a chatbot's conversation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser # Import the string output parser\n",
    "# Create an instance of the string output parser\n",
    "output_parser=StrOutputParser()\n",
    "chain=prompt| llm | output_parser # Combine the prompt template, language model, and output parser\n",
    "response=chain.invoke({\"input\":\"Can you please explain to a Computer science student, about Chat prompt Template in simpler words\"})\n",
    "# Print the result of the chain invocation\n",
    "print(response) \n",
    "# Print the type of the response\n",
    "type(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
